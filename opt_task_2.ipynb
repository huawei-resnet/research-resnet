{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding Project: Deep Learning Basics\n",
    "\n",
    "* ### Based on the paper:K. He, X. Zhang, S. Ren and J. Sun, “Deep Residual Learning for Image Recognition,” IEEE Conference on Computer Vision and Pattern Recognition (CVPR),2016.\n",
    "\n",
    "* ### Assignment\n",
    "\n",
    "  1. Get familiar with our coding environment (on cloud)!\n",
    "  2. Find a codebase of this paper, download the CIFAR10 and CIFAR100 datasets\n",
    "  3. Run the basic code on the server, with deep residual networks with 20, 56 and 110 layers, and obtain results (3-time average) on both CIFAR10 and CIFAR100\n",
    "  4. Finish the required task and one of the optional tasks (see the following slides) –of course, you can do more than one optional tasks if you wish (bonus points)\n",
    "  5. If you have more ideas, please specify a new task by yourself (bonus points)\n",
    "  6. Remember: integrate your results into your reading report\n",
    "  7. Submit your report(as PDF) and code (as README doc) on the iLearningX: https://ilearningx-ru.huaweiuniversity.com/courses/course-v1:HuaweiX+WHURU001+Self-paced/courseware/8825cc7815fa444696520baaf31fa2b0/77b7babd6ae34949bc209d7a8f0ba409/(8)  \n",
    "\n",
    "Date assigned: Oct. 15, 2019;    Date Due: Dec 31, 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Task 2\n",
    "* Modifying network architecture\n",
    "    * Based on the results of the basic (required) experiments\n",
    "    * How does the change of network structure impact final performance?\n",
    "* Questions to be discussed in the report\n",
    "    * What if we adjust the number of residual blocks in different stages? For fair comparison, please keep the number of residual blocks unchanged\n",
    "    * What if we train residual networks with 50 or 62 layers? How do they compare against the network with 56 layers? 3-time average required!\n",
    "    * What if we remove all skip connections in residual networks? What if we add a skip connection after each 1 or 3 (not 2) convolutional layers? For fair comparison, please keep the number of convlayers unchanged\n",
    "    * Note: do not simply report accuracy, discussion on reasons is expected!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "One time installation of required libraries from requirement.txt and creating data path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt\n",
    "!mkdir data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading CIFAR10 and CIFAR100 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.dataset_dowloader_ import *\n",
    "\n",
    "cifar10_dowloader()\n",
    "cifar100_dowloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The basic training and testing pipeline\n",
    "### What if we adjust the number of residual blocks in different stages? For fair comparison, please keep the number of residual blocks unchanged.\n",
    "* `layer_values = [[3, 3, 3], [1, 3, 5], [5, 3, 1], [3, 1, 5]]` - define ResNet20 by different number of layers on each of 3 stages\n",
    "* `history_stages = []` - define train/validation logs' container\n",
    "* `auto_resnet(layer_j, 100, 1, 180, history_stages)`:\n",
    "    * `100` - CIFAR100 dataset\n",
    "    * `1` - learning rate multiplier (base learning rate is 1*0.1)\n",
    "    * `180` - number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from auto_resnet import * \n",
    "\n",
    "layer_values = [[3, 3, 3], [1, 3, 5], [5, 3, 1], [3, 1, 5]] # 20\n",
    "history_stages = []\n",
    "\n",
    "for layer_j in layer_values:\n",
    "    auto_resnet(layer_j, 100, 1, 180, history_stages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "legend_stages = ['train_r20_333', 'train_r20_135', 'train_r20_531', 'train_r20_315']\n",
    "\n",
    "plt_different_history(history_stages, legend_stages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The basic training and testing pipeline\n",
    "### What if we train residual networks with 50 or 62 layers? How do they compare against the network with 56 layers?\n",
    "* `layer_values = [[9, 9, 9], [8, 8, 8], [10, 10, 10]]` - define ResNet56 & ResNet50 & ResNet62\n",
    "* `history_50_62 = []` - define train/validation logs' container\n",
    "* `auto_resnet(layer_j, 100, 1, 180, history_50_62)`:\n",
    "    * `100` - CIFAR100 dataset\n",
    "    * `1` - learning rate multiplier (base learning rate is 1*0.1)\n",
    "    * `180` - number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from auto_resnet import * \n",
    "\n",
    "layer_values = [[9, 9, 9], [8, 8, 8], [10, 10, 10]] # 56, 50, 62\n",
    "history_50_62 = []\n",
    "\n",
    "for layer_j in layer_values:\n",
    "    auto_resnet(layer_j, 100, 1, 180, history_50_62)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "legend_50_62 = ['train_r56', 'train_r50', 'train_r62']\n",
    "\n",
    "plt_different_history(history_50_62, legend_50_62)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The basic training and testing pipeline\n",
    "### What if we remove all skip connections in residual networks? What if we add a skip connection after each 1 or 3 (not 2) convolutional layers? For fair comparison, please keep the number of convlayers unchanged.\n",
    "* `rm_conn_values = [True, False]` - define skipconnection flags (True - remove all skip connections in residual network, False - leave all skip connections in residual network; base flaf is False)\n",
    "* `conv_num_values = [1, 2, 3]` - define number of conv layers inside residual block (base number is 2)\n",
    "* `history_structure = []` - define train/validation logs' container\n",
    "* `auto_resnet([3,3,3], 100, 1, 180, history_structure, 0.8, 64, conv_num_j, rm_conn_i)`:\n",
    "    * `[3,3,3]` - ResNet20 model\n",
    "    * `100` - CIFAR100 dataset\n",
    "    * `1` - learning rate multiplier (base learning rate is 1*0.1)\n",
    "    * `180` - number of epochs\n",
    "    * `0.8` - data parts multiplier (base ratio train/validation sub datasets is 0.8/0.2)\n",
    "    * `64` - batch size valuse (base batch size is 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from auto_resnet import * \n",
    "\n",
    "rm_conn_values = [True, False]\n",
    "conv_num_values = [1, 2, 3]\n",
    "history_structure = []\n",
    "\n",
    "for rm_conn_i in class_values:\n",
    "    for conv_num_j in layer_values:\n",
    "        auto_resnet([3,3,3], 100, 1, 180, history_structure, 0.8, 64, conv_num_j, rm_conn_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "legend_50_62 = ['skip_1', 'skip_2', 'skip_3', 'no_skip_1', 'no_skip_2', 'no_skip_3']\n",
    "plt_different_history(history_structure, legend_50_62)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch-1.0.0",
   "language": "python",
   "name": "pytorch-1.0.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
