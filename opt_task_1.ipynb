{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding Project: Deep Learning Basics\n",
    "\n",
    "* ### Based on the paper:K. He, X. Zhang, S. Ren and J. Sun, “Deep Residual Learning for Image Recognition,” IEEE Conference on Computer Vision and Pattern Recognition (CVPR),2016.\n",
    "\n",
    "* ### Assignment\n",
    "\n",
    "  1. Get familiar with our coding environment (on cloud)!\n",
    "  2. Find a codebase of this paper, download the CIFAR10 and CIFAR100 datasets\n",
    "  3. Run the basic code on the server, with deep residual networks with 20, 56 and 110 layers, and obtain results (3-time average) on both CIFAR10 and CIFAR100\n",
    "  4. Finish the required task and one of the optional tasks (see the following slides) –of course, you can do more than one optional tasks if you wish (bonus points)\n",
    "  5. If you have more ideas, please specify a new task by yourself (bonus points)\n",
    "  6. Remember: integrate your results into your reading report\n",
    "  7. Submit your report(as PDF) and code (as README doc) on the iLearningX: https://ilearningx-ru.huaweiuniversity.com/courses/course-v1:HuaweiX+WHURU001+Self-paced/courseware/8825cc7815fa444696520baaf31fa2b0/77b7babd6ae34949bc209d7a8f0ba409/(8)  \n",
    "\n",
    "Date assigned: Oct. 15, 2019;    Date Due: Dec 31, 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Task 1\n",
    "\n",
    "* Changing hyper-parameters\n",
    "    * Based on the results of the basic (required) experiments\n",
    "    * How does the change of hyper-parameters impact final performance?\n",
    "* Questions to be discussed in the report\n",
    "    * What if we multiply the base learning rate by 10, 5, 2, or by 1/10, 1/5, 1/2?\n",
    "    * What if we double the number of training epochs? What if we half it? Note that the learning rate policy should be adjusted accordingly (please specify details)\n",
    "    * What if we only use 1/2 or 1/5 of training data? What if we double or half the size of mini-batch? Note that for fair comparison, you need to keep the number of training samples (iterations x batchsize) unchanged\n",
    "    * Note: do not simply report accuracy, discussion on reasons is expected!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "One time installation of required libraries from requirement.txt and creating data path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt\n",
    "!mkdir data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading CIFAR10 and CIFAR100 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.dataset_dowloader_ import *\n",
    "\n",
    "cifar10_dowloader()\n",
    "cifar100_dowloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The basic training and testing pipeline\n",
    "\n",
    "### What if we multiply the base learning rate by 10, 5, 2, or by 1/10, 1/5, 1/2?\n",
    "\n",
    "* `lr_multipliers = [1, 10, 5, 2, 0.1, 0.2, 0.5]` - define learning rate multipliers (base learning rate is 1*0.1)\n",
    "* `history_lr = []` - define train/validation logs' container\n",
    "* `auto_resnet(20, 100, lr_i, 180, history_lr)`:\n",
    "    * `[3, 3, 3]` - ResNet20 model\n",
    "    * `100` - CIFAR100 dataset\n",
    "    * `lr_i` - learning rate multiplier\n",
    "    * `180` - number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from auto_resnet import *\n",
    "\n",
    "lr_multipliers = [1, 10, 5, 2, 0.1, 0.2, 0.5]\n",
    "history_lr = []\n",
    "for lr_i in lr_multipliers:\n",
    "    # resnet20, cifar100\n",
    "    auto_resnet([3, 3, 3], 100, lr_i, 180, history_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "legend_lr = ['lr_1', 'lr_10', 'lr_5', 'lr_2', 'lr_0.1', 'lr_0.2', 'lr_0.5']\n",
    "plt_different_history(history_lr, legend_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The basic training and testing pipeline\n",
    "\n",
    "### What if we double the number of training epochs? What if we half it? **Note that the learning rate policy should be adjusted accordingly (please specify details)**\n",
    "\n",
    "* `epochs_multipliers = [1, 2, 0.5]` - define epochs multipliers (base epoch is 180)\n",
    "* `history_lr = []` - define train/validation logs' container\n",
    "* `auto_resnet(20, 100, 1, 180/epoch_m, history_epochs)`:\n",
    "    * `[3, 3, 3]` - ResNet20 model\n",
    "    * `100` - CIFAR100 dataset\n",
    "    * `epoch_m` - learning rate multiplier (base learning rate is 1*0.1)\n",
    "    * `180/epoch_m` - number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from auto_resnet import *\n",
    "\n",
    "epochs_multipliers = [1, 2, 0.5]\n",
    "history_epochs = []\n",
    "\n",
    "for epoch_m in epochs_multipliers:\n",
    "    auto_resnet([3, 3, 3], 100, epoch_m, 180/epoch_m, history_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "legend_epochs = ['epoch_/1', 'epoch_/2', 'epoch_/0.5']\n",
    "plt_different_history(history_epochs, legend_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The basic training and testing pipeline\n",
    "\n",
    "### What if we only use 1/2 or 1/5 of training data? What if we double or half the size of mini-batch? Note that for fair comparison, you need to keep the number of training samples (iterations x batchsize) unchanged\n",
    "\n",
    "* `data_part_values = [0.8, 0.5, 0.2]` - define data parts multipliers (base ratio train/validation sub datasets is 0.8/0.2)\n",
    "* `batch_sizes = [128, 64, 32]` - define batch size valuses (base batch size is 64)\n",
    "* `history_batch = []` - define train/validation logs' container\n",
    "* `auto_resnet([3, 3, 3], 100, 1, 180, history_batch, data_part_i, batch_i)`:\n",
    "    * `[3, 3, 3]` - ResNet20 model\n",
    "    * `100` - CIFAR100 dataset\n",
    "    * `1` - learning rate multiplier\n",
    "    * `180` - number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from auto_resnet import *\n",
    "\n",
    "data_part_values = [0.8, 0.5, 0.2]\n",
    "batch_sizes = [128, 64, 32]\n",
    "history_batch = []\n",
    "\n",
    "for data_part_i in data_part_values:\n",
    "    for batch_i in batch_sizes:\n",
    "        auto_resnet([3, 3, 3], 100, 1, 180, history_batch, data_part_i, batch_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "legend_batch = ['0.8x128', '0.8x64', '0.8x32', '0.5x128', '0.5x64', '0.5x32', '0.2x128', '0.2x64', '0.2x32']\n",
    "plt_different_history(history_batch, legend_batch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch-1.0.0",
   "language": "python",
   "name": "pytorch-1.0.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
